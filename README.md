# Applied_Finance_project
I downloaded tick-by-tick data from Truefx - a website recommended in the lecture slides. With 3 million rows and data exhibiting high noise. I decided to aggregate the dataset to 1 minutes
Thanks to time-aggregation, I could compute the number of ticks per minute - a feature representing market volume in this context. Additionally, I computed all the OHLC quotes for both BID and ASK. Here, I made a simplifying assumption and I set a “middle average price per minute” representing (average_ask - average_bid) / 2. So for each row I obtained one universal price.
I was very cautious about missing values - I needed to exclude everything in the time range Fridays 5 P.M. - Sundays P.M. Additionally I excluded the last 5 minutes before market closing (16 55 Fridays) for “safety” reasons (I could have also hardcoded this time as flat position, but I chose to drop) and the first 10 minutes on Sunday 17 00 - 17 10 - not really for safety reasons - I input lagged features to 10th period, so I would get some NAs or last values that occurred on Friday which makes no sense
The most important feature in my case was log returns of my aferomentionned universal minute price - middle average price per minute (ln_mid_returns). 
Before going further wit feature engineering, I made train/test split. It was essential to avoid data leakage which might have occurred if I added any lagged features before the split. 
I added features related to OHLC prices log returns (and their lags), 2 dummy variables for Friday and Sunday. I made 10 lags - 1,2,3…10 for OHLC and askbid average spread. 
Target variable: the most important part. I decided to prioritize the log returns 3 minutes later. So if at minute t the algorithm predicts ln_mid_returns higher than the threshold (half of the absolute value of the median of n_mid_returns from the train sample), it will choose to go long (1). If it’s lower than 0,4median (yes, I penalize the losses more) it goes short. Otherwise - stay flat. Important thing to add, that the transaction is executed 3 minutes later from P&L perspective. I was predicting the direction of the price in 3 minutes not for multiplying my signals at the next minute, but really at the time for which the prediction was made. 
Finally, I trained the LightGBM algorithm. I obtained 0.1028 of Sharp Ratio which of course is not a brilliant score. For Profit and Loss, I assumed 100 000 of initial capital and did the multiplication I explained above (signal at t * log return at t+3). I received around 392 PLN profit, so 0,4%. In addition, please find attached my Equity Curve.  I must say that I excluded any transaction or slippage costs from my model.
The model’s weaknesses? Very simplistic assumption about prices (I just computed the middle price without saying to the algorithm buy at ask price and sell at bid one). I also could have tried more algorithms, more combinations of lags/which log return to predict or include transaction costs to be more realistic. Also another time aggregation could give another interesting insight. I gave up technical indicators because according to my research on the internet - they are not useful with such noisy data - but I could have tried on my own some moving averages. I also gave up volatility modelling with GARCH-familly models, because I did not feel confident about them. Well, lesson for the future…

